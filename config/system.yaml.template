# System configuration for Chorus Engine
# This file is optional - defaults will be used if not present

# ===========================
# LLM Provider Configuration
# ===========================

# Option 1: Ollama (recommended - multi-model, auto-loading, easy GGUF import)
# Download models via Model Manager UI, they auto-import to Ollama
llm:
  provider: ollama
  base_url: http://localhost:11434
  model: ""  # Empty on fresh install - download a model via Model Manager and click "Switch To"
  context_window: 8192  # Safe default - adjust based on your model's capabilities
  max_response_tokens: 4096
  temperature: 0.7
  archivist_model: "qwen3:4b-instruct"  # Ollama model name for archivist analysis (summary + memories)
                              # LM Studio: use the exact LM Studio model name if provider=lmstudio
                              # KoboldCpp: leave blank (single-model runtime)
                              # VRAM note: set to same as llm.model for maximum VRAM management
                              # Recommended: keep qwen3:4b (tested archivist performance)
  analysis_max_tokens_summary: 4096  # Max tokens for conversation summary analysis
  analysis_max_tokens_memories: 4096  # Max tokens for memory extraction analysis
  analysis_min_tokens_summary: 500  # Minimum tokens required for summary analysis
  analysis_min_tokens_memories: 0  # Minimum tokens required for memory extraction analysis
  timeout_seconds: 120
  unload_during_image_generation: true

# Option 2: LM Studio (multi-model, JIT loading)
# llm:
#   provider: lmstudio
#   base_url: http://localhost:1234 #/v1
#   model: qwen/qwen2.5-coder-14b
#   context_window: 31500  # IMPORTANT: Must match the context length set in LM Studio when loading the model!
#                          # LM Studio default is often 4096 - change this when loading your model
#                          # Recommended: 31500 (for 32K context supporting model) for rich conversations with documents and memory
#   max_response_tokens: 4096
#   temperature: 0.7
#   timeout_seconds: 120
#   unload_during_image_generation: true

# Option 3: KoboldCpp (single-model, manual loading)
# User must start KoboldCpp separately with:
#   koboldcpp.exe --model path/to/model.gguf --port 5001 --contextsize 8192 --gpulayers 99
# Note: All characters will use the same loaded model (preferred_llm_model ignored)
# Note: Cannot unload for ComfyUI (VRAM shared between KoboldCpp and ComfyUI)
# llm:
#   provider: koboldcpp
#   base_url: http://localhost:5001
#   model: "qwen-14b"  # Label for logging (actual model loaded at KoboldCpp startup)
#   context_window: 28672  # MUST match --contextsize flag in KoboldCpp startup
#   max_response_tokens: 4096
#   temperature: 0.7
#   timeout_seconds: 120
#   unload_during_image_generation: false  # Cannot unload KoboldCpp models dynamically

memory:
  embedding_model: all-MiniLM-L6-v2
  vector_store: chroma
  similarity_thresholds:
    explicit_minimum: 0.70
    implicit_minimum: 0.75
    search_api_minimum: 0.65
  default_budget_tokens: 1000

comfyui:
  enabled: true
  url: http://localhost:8188
  timeout_seconds: 300  # Image generation timeout
  video_timeout_seconds: 600  # Video generation timeout (10 minutes)
  polling_interval_seconds: 2.0
  max_concurrent_jobs: 2

# In-conversation media tooling and proactive offer policy
media_tooling:
  enabled: true
  offers_enabled: true
  image_offers_enabled: true
  video_offers_enabled: true
  explicit_min_confidence_image: 0.50
  explicit_min_confidence_video: 0.45
  offer_min_confidence_image: 0.50
  offer_min_confidence_video: 0.45
  offer_cooldown_minutes: 30
  offer_min_turn_gap: 8
  max_offers_per_conversation_per_media: 2
  disable_offers_for_sources: [discord]

# TTS (Text-to-Speech) Configuration
tts:
  default_provider: chatterbox  # Options: chatterbox (embedded, fast) or comfyui (workflow-based)
                                # Characters can override this in their voice config

# Vision System Configuration
# Enables characters to "see" and analyze images uploaded by users
# Uses vision-language models (VLMs) through your configured LLM provider
# Backend automatically matches llm.provider (ollama, lmstudio, etc.)
vision:
  enabled: true  # Enable/disable vision capability system-wide
  
  # Model Configuration
  model:
    name: qwen3-vl:4b  # Vision model to use (loaded via llm.provider backend)
                       # Ollama: Download with: ollama pull qwen3-vl:4b
                       # LM Studio: Use model search to download qwen2-vl models
                       # Options: qwen3-vl:2b (fastest), qwen3-vl:4b (balanced), 
                       #          qwen3-vl:8b (higher quality), qwen3-vl:30b (best)
                       # Recommendation: 4b for most users (5-12s processing, excellent quality)
    load_timeout_seconds: 60  # Time to wait for vision model to load
  
  # Image Processing Settings
  processing:
    max_retries: 2  # Number of retries if vision analysis fails
    timeout_seconds: 30  # Timeout for vision analysis (per attempt)
    resize_target: 768  # Max dimension (width or height) for processing
                         # Images larger than this are resized to reduce processing time
    supported_formats:  # Allowed image file types
      - jpg
      - jpeg
      - png
      - webp
      - gif
    max_file_size_mb: 10  # Maximum upload size (MB)
  
  # Output Configuration
  output:
    format: structured  # Output format: structured (JSON with details) or minimal
    include_confidence: true  # Include confidence score in vision results
  
  # Memory Integration
  # Visual memories allow characters to recall images shown to them
  memory:
    auto_create: true  # Automatically create memories for analyzed images
    category: visual  # Memory category for visual observations
    default_priority: 70  # Priority level for visual memories (0-100)
    min_confidence: 0.6  # Minimum vision confidence to create memory
  
  # Intent Detection (for bridges like Discord/Slack)
  intent:
    bridge_always_analyze: false  # If true, analyze ALL images from bridges
    bridge_never_analyze: false  # If true, NEVER analyze bridge images
    web_ui_always_analyze: true  # Web UI always analyzes (explicit upload)
    use_semantic_detection: false  # Use semantic intent detection (Phase 3-4)
    confidence_threshold: 0.45  # Threshold for semantic intent detection
    
    # Phrases that trigger vision analysis in bridge messages
    # If user message contains these, image will be analyzed
    trigger_phrases:
      - what do you see
      - look at
      - what's in
      - describe
      - check this
      - what does this
      - can you see
      - tell me about
      - what is this
      - help with this
      - analyze
      - show me
  
  # Performance & Caching
  cache:
    enabled: true  # Cache vision analysis results in database
    allow_reanalysis: true  # Allow re-analyzing images with different models
  
  # VRAM Management
  # Note: Ollama and LM Studio automatically handle VRAM allocation between models
  # The vision model will be unloaded when ComfyUI workflows run (see llm.unload_during_image_generation)
  # No manual VRAM management needed - the backends handle this intelligently

# Document Analysis System
document_analysis:
  enabled: true
  default_max_chunks: 3              # Fallback if token calculation unavailable
  max_chunks_cap: 25                 # Absolute safety limit
  chunk_token_estimate: 512          # Average chunk size for calculations
  document_budget_ratio: 0.15        # 15% of available context for documents

# Conversation Memory Enrichment
# Allows characters to recall and reference relevant past conversations
# Uses semantic search to find past conversations matching the current topic
conversation_context:
  enabled: true                      # Enable/disable conversation context retrieval
  
  # Similarity thresholds (0.0-1.0, higher = more strict matching)
  passive_threshold: 0.75            # Used when user doesn't explicitly ask about past
                                     # High bar ensures only highly relevant context appears
  triggered_threshold: 0.55          # Used when user asks "remember when..." etc.
                                     # Lower bar since user is explicitly requesting recall
  
  max_summaries: 2                   # Maximum past conversations to include in context
  token_budget_ratio: 0.05           # Portion of context window for summaries (5%)
                                     # Unused budget cascades to memory retrieval

# Background Processing (Heartbeat System)
# Runs automatic tasks during system idle time, never interfering with user activity
# Initial use case: Auto-analyze stale conversations for searchable summaries
heartbeat:
  enabled: true                      # Enable/disable background processing
  interval_seconds: 60               # How often to check for idle state (seconds)
  
  # Idle Detection - when to consider system "safe" for background work
  idle_threshold_minutes: 5          # Minutes of no activity before processing starts
  resume_grace_seconds: 2            # Pause duration after activity resumes before
                                     # background processing can start again
  
  # Conversation Summary Analysis Settings (stricter)
  analysis_summary_stale_hours: 24   # Hours since last activity before eligible
  analysis_summary_min_messages: 10  # Minimum messages required
  analysis_summary_batch_size: 3     # Summaries to process per batch

  # Conversation Memory Analysis Settings (can be more frequent)
  analysis_memories_stale_hours: 24  # Hours since last activity before eligible
  analysis_memories_min_messages: 10 # Minimum messages required
  analysis_memories_batch_size: 3    # Memory extractions per batch
  
  # GPU Utilization Check (NVIDIA only, optional)
  # Final safety check before background tasks - detects external GPU activity
  # (gaming, training, external ComfyUI/Ollama, etc.)
  gpu_check_enabled: false           # Enable GPU utilization check
  gpu_max_utilization_percent: 15    # Skip tasks if GPU busier than this %
  
  # Scheduled Character Backups (idle-only, optional per character)
  backups:
    enabled: true                    # Enable heartbeat backup scheduling engine
    interval_hours: 24               # Target cadence for due checks and run frequency
    destination_dir: null            # ABSOLUTE path required to write scheduled backup archives
    retention_days: 14               # Prune scheduled archives older than this many days
    max_backups_per_cycle: 2         # Maximum scheduled backup jobs processed per heartbeat cycle
    skip_if_unchanged: true          # Skip if manifest fingerprint unchanged since last success
    verify_archive: true             # Validate archive integrity with zipfile test

# Startup synchronization
startup:
  sync_summary_vectors: true         # Auto-sync missing conversation summaries on startup
                                     # Provides self-healing after restore, migration, or corruption
  sync_memory_vectors: true          # Auto-sync memory vectors on startup
  sync_moment_pin_vectors: true      # Auto-sync moment pin vectors on startup

paths:
  characters: characters
  workflows: workflows
  data: data

ui:
  color_scheme: stage-night  # Options: stage-night, spotlight-bright, noir-absolute, cobalt-depths, 
                              #          forest-canopy, ember-glow, monochrome-studio, sunset-boulevard, arctic-twilight

api_host: localhost
api_port: 8080
debug: true
