# System configuration for Chorus Engine
# This file is optional - defaults will be used if not present

# llm:
#   provider: ollama
#   base_url: http://localhost:11434
#   model: qwen2.5:14b-instruct
#   context_window: 32768  # Qwen2.5 supports much larger context
#   max_response_tokens: 2048
#   temperature: 0.7
#   timeout_seconds: 120
  
#   unload_during_image_generation: true

llm:
  provider: lmstudio
  base_url: http://localhost:1234 #/v1
  model: qwen/qwen2.5-coder-14b
  context_window: 32768  # Qwen2.5 supports much larger context
  max_response_tokens: 2048
  temperature: 0.7
  timeout_seconds: 120
  
  unload_during_image_generation: true

memory:
  embedding_model: all-MiniLM-L6-v2
  vector_store: chroma
  implicit_enabled: false
  ephemeral_ttl_hours: 24
  similarity_thresholds:
    explicit_minimum: 0.70
    implicit_minimum: 0.75
    search_api_minimum: 0.65
  default_budget_tokens: 1000

comfyui:
  enabled: true
  url: http://localhost:8188
  timeout_seconds: 300
  polling_interval_seconds: 2.0
  max_concurrent_jobs: 2

# Intent Detection System (Phase 7)
intent_detection:
  enabled: false  # Using keyword-based detection (no LLM overhead)
  model: gemma2:9b  # Better at structured output than small models
  temperature: 0.2  # Low for consistency, but not too low to avoid token corruption
  keep_loaded: true
  fallback_to_keywords: true
  
  # Intent-specific confidence thresholds
  thresholds:
    image: 0.7
    video: 0.7
    memory: 0.8
    ambient: 0.6

# VRAM Management (Phase 7)
vram_management:
  # Unload all models during ComfyUI generation
  unload_during_comfy: true
  
  # Models to reload after generation
  always_reload:
    - intent_model
    - character_model
  
  # Verification
  verify_unload: true
  verify_reload: true

paths:
  characters: characters
  workflows: workflows
  data: data

debug: true
api_host: localhost
api_port: 8080
